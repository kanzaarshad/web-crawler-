{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-3o2MV58y_n"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Function to fetch and parse a webpage\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check if the request was successful\n",
        "        return BeautifulSoup(response.text, 'html.parser')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract all URLs from a webpage\n",
        "def extract_urls(soup, base_url):\n",
        "    urls = set()\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        full_url = urljoin(base_url, href)  # Convert relative URLs to absolute\n",
        "        urls.add(full_url)\n",
        "    return urls\n",
        "\n",
        "# Function to crawl a given URL\n",
        "def crawl(url, max_pages=10):\n",
        "    pages_crawled = 0\n",
        "    urls_to_crawl = set([url])\n",
        "    crawled_urls = set()\n",
        "\n",
        "    while urls_to_crawl and pages_crawled < max_pages:\n",
        "        current_url = urls_to_crawl.pop()\n",
        "        if current_url in crawled_urls:\n",
        "            continue\n",
        "\n",
        "        print(f\"Crawling: {current_url}\")\n",
        "        soup = fetch_page(current_url)\n",
        "        if soup:\n",
        "            new_urls = extract_urls(soup, current_url)\n",
        "            urls_to_crawl.update(new_urls - crawled_urls)\n",
        "            crawled_urls.add(current_url)\n",
        "            pages_crawled += 1\n",
        "\n",
        "    print(\"\\nCrawling finished.\")\n",
        "    print(f\"Pages crawled: {pages_crawled}\")\n",
        "    print(f\"Total unique URLs found: {len(crawled_urls)}\")\n",
        "\n",
        "# Main function to start the crawler\n",
        "if __name__ == \"__main__\":\n",
        "    start_url = \"https://example.com\"  # Replace with the URL you want to crawl\n",
        "    crawl(start_url)\n"
      ]
    }
  ]
}